\relax 
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\citation{mnih-dqn-2015}
\citation{44806}
\citation{DBLP:journals/corr/RezendeEMBJH16}
\@writefile{toc}{\contentsline {section}{Introduction}{3}}
\@writefile{toc}{\contentsline {section}{Motivation}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Problem definition}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Simple case}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Forumlation of Reinforcement Learning Problem}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Definitions}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Theorems}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Monte Carlo Algorithm}{6}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Greedy in the Limit with Infinite Exploration Algorithm (GLIE)\relax }}{6}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:glie}{{1}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}SARSA Algorithm}{7}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces SARSA Algorithm for On-Policy Control\relax }}{7}}
\newlabel{alg:sarsa}{{2}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}SARSA-$\lambda $ Algorithm}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Action Value Function Approximation}{7}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces SARSA-$\lambda $ Algorithm for On-Policy Control\relax }}{8}}
\newlabel{alg:sarsa_lambda}{{3}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Reinforcement Learning applied to Blackjack}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Experimental results}{9}}
\@writefile{toc}{\contentsline {subsubsection}{Case the score upper limit is 21}{9}}
\citation{mnih-dqn-2015}
\@writefile{toc}{\contentsline {subsubsection}{Case the upper limit is 99}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Further analysis}{10}}
\citation{mnih-dqn-2015}
\@writefile{toc}{\contentsline {subsubsection}{Figures}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusion}{11}}
\newlabel{E21_MC_V}{{\caption@xref {E21_MC_V}{ on input line 388}}{12}}
\newlabel{sub@E21_MC_V}{{}{12}}
\newlabel{E21_S_V}{{1b}{12}}
\newlabel{sub@E21_S_V}{{b}{12}}
\newlabel{E21_S_V}{{1c}{12}}
\newlabel{sub@E21_S_V}{{c}{12}}
\newlabel{E21_SL_V}{{1d}{12}}
\newlabel{sub@E21_SL_V}{{d}{12}}
\newlabel{E21_S_V}{{1e}{12}}
\newlabel{sub@E21_S_V}{{e}{12}}
\newlabel{E21_SL_LA_V}{{1f}{12}}
\newlabel{sub@E21_SL_LA_V}{{f}{12}}
\newlabel{E21_S_V}{{1g}{12}}
\newlabel{sub@E21_S_V}{{g}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Optimal Value functions after 10\IeC {\textsuperscript  6} episodes for the case of the upper bound of 21, and the corresponding RMSE against Monte Carlo Optimal Value functions with a step-size of 1000\relax }}{12}}
\newlabel{fig:fig}{{1}{12}}
\citation{*}
\bibstyle{plain}
\bibdata{bibliography}
\bibcite{Lazaric07reinforcementlearning}{1}
\bibcite{mnih-dqn-2015}{2}
\bibcite{DBLP:journals/corr/RezendeEMBJH16}{3}
\bibcite{44806}{4}
\bibcite{Sutton:1998:IRL:551283}{5}
\@writefile{toc}{\contentsline {section}{References}{13}}
